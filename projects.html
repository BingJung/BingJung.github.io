<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Past Projects | Bingjun Guo</title>
    <link rel="stylesheet" href="my_stylesheet.css">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js"></script>
</head>

<body>

<header>
    <nav class="subpages">
        <ul>
            <li class="main"><a href = "index.html">Bingjun Guo</a></li>            
            <li class="section"><a href = "acknowledgements.html">Acks</a></li>
            <li class="section"><a href = "babblings.html">Babblings</a></li>
            <li class="section"><a href = "courses.html">Courses</a></li>
        </ul>
    </nav>
</header>

<main>
<div class="wrapper">
    <div class="intro">
        <h1>Past Projects</h1>
        <p>This is an incomplete page of the projects that I have worked on. Most of them were accomplished in cooperation with my wonderful classmates.
    </div>
    <div class="project">
        <h2><a href="secrets/489-final-project.pdf">Variational Reasoning with Parallelogram Analogy</a></h2>
        <div class="info">
            Course Project, Spring 2024 (junior year)
        </div>
        <p>
            At first I was trying to give a rigorous explanation here, then I referred to prior and posterior probablities, then I turned to figure out how they could be correlated with Kant's a priori and a posteriori knowledge (<span class="whisper">although I knew there were possibly no correlations</span>), and then I started to wonder if Kant's synthetic a priori really exists or we could somehow disprove it, which was kind of relevant to how far we could and shall push ML models with current data as well as where we were. I think I'm kind of stuck now, and you might just refer to the brief report linked to the subtitle for this project before I figure out or forget all these stuffs.
        </p>
    </div>
    <div class="project">
        <h2>A Unix-like Operating System</h2>
        <div class="info">
            Course Project, Fall 2023 (junior year)
        </div>
        <p>
            This project marks almost an end of our journey from hardware to software. In group of four, we built the kernel of this mini multitask terminal-interactive x86 operating system. I was mainly responsible for building terminals and assembly linkages, which required sufficient familiarity with all features of our operating system, including interrupts, scheduling, virtual memory, and file system. From my perspective of view, an operating system could be all about synchronization and interaction, which were equally important when switching among multiple terminals. We also added support for command history, auto command completion, terminal color switch, and a mouse cursor in our kernel. Frankly speaking these were likely to be just above normal, as there were some groups doing seriously marvelous jobs.</p>
    </div>
    <div class="project">
        <h2>A LLM Augmented Data Cleaning Framework</h2>
        <div class="info">
            Summer Research Project, Summer 2023 (sophomore year)
        </div>
        <p>
            When this project was handed us, it was just a few keywords, and we were just with a few blank minds. After our exploration from RNNs to Transformer, we were offered two papers by the mentors, one of which introduced a Transformer model representing heterogenous tabular data while the other displayed success on predicting time-series data with parameters of GPT2. To be honest I really wished to do something fancier at first, but both time and resources was rather limited. Hence, after getting familiar with the codes, what I did was simply plugging the BERT parameters in the former work and praying for a better performance on representing dirty data. <br><br>
            Then I did a number of experiments under different frozen conditions. Finetuning on norm layers barely changed anything, while finetuning full parameteres appeared easily overfitting on most data that were limited in features or size. Fortunately the results did seem better when applied on datasets with sufficiently large amount of features (50~200) and samples. The reason why it only "seemed" so was that the finetuning took incredibly long on a single A40 and there was no time left for the comparison tests on the original model with same or larger amount of parameters. Thus, we just left the job as explanations in our report and headed for our new semester which was rather vital and challenging.
        </p>
    </div>
    <div class="project">
        <h2></h2>
        <p>
            
        </p>
    </div>
    <div class="project">
        <a href="secrets/ECE_110_final_report.pdf"><h2>A Processor-Free Intelligent Car</h2></a>
        <div class="info">
            Course Project, Fall 2021 (freshman year)
        </div>
        <p>
            This is the project that brought my aspiration. The car follows a black path on its own and uses an ultrasonic sensor to keep a distance from obstacles. Where we were quite proud of it was rather than relying on programmed Arduino, we realized such functions with logic circuits built with diodes, resistors, capcitors, MOSFETs, and some other simple elements. We made the car processed signals and applied control with a mixture of digital & analog signals, and it was said that we were the only group achieving so. I hope I can do the same thing to the development of AI.
        </p>
    </div>
</div>
</main>
<footer>
    <p>
        &copy; 2024 Bingjun Guo
    </p>
</footer>

</body>

</html>